{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexfrater/deep-learning-course/blob/master/07_VAE_GAN_AW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtCVMgi8iygt"
      },
      "source": [
        "# Variational Autoencoders\n",
        "\n",
        "<a href=\"https://ibb.co/GMMVqft\"><img src=\"https://i.ibb.co/X55zqf3/vae.jpg\" alt=\"vae\" border=\"0\"></a>\n",
        "\n",
        "Image taken from [here](http://kvfrans.com/variational-autoencoders-explained/)\n",
        "\n",
        "In the autoencoder tutorial we showed how to learn a meaningful representation of the data by using an autoencoder. In an autoencoder, the input image was transformed into a vector which encoded the information from the image in a lower dimensionality space. Then, we decoded that vector to get a reconstruction of the input image. However, the model was focused on encoding existing data for representation learning or similar purposes. To tackle the generation of new data, we will use a Variational Autoencoder (VAE) approach. The image shows an overview of the VAE method.\n",
        "\n",
        "Parts of the code are taken from [here](https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/), which contains a more in-depth explanation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNlgJvqul7W2"
      },
      "source": [
        "Before starting to define the different parts of the VAE, let's import the needed modules for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91g-k-uqdRW7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "np.random.seed(123)  # for reproducibility\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Layer, Input, Lambda\n",
        "from keras.layers import Multiply, Add, BatchNormalization, Reshape\n",
        "from keras.layers import UpSampling2D, Convolution2D, LeakyReLU, Flatten, ReLU\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from scipy.stats import norm\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import sys\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "from keras import initializers\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndKHD2XrmDej"
      },
      "source": [
        "Now we load MNIST, which will be our toy dataset for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY6F0Ln8czZA"
      },
      "outputs": [],
      "source": [
        "original_dim = 784\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, original_dim) / 255.\n",
        "x_test = x_test.reshape(-1, original_dim) / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9_UOneRBn1q"
      },
      "source": [
        "Now let's do a quick recap of the Variational AutoEncoder (VAE) theory. As stated in the lecture, we want to find the $\\hat{\\theta}$ that approximates $P_\\theta(x)$ by doing:\n",
        "\n",
        "$$\n",
        "\\hat{\\theta} = \\text{argmax}_\\theta \\sum_{i=1}^n \\mathbb{E}_{Q_\\phi(z|x_i)}[\\log(P_\\theta(x_i|z))] - \\text{KL}(Q_\\phi(z|x_i) || P_\\theta(z))\n",
        "$$\n",
        "Se will minimize the negative of that term in order to maximize it, where\n",
        "we will train our model using a stochastic approach by sampling mini-batches from the dataset. First, we define the two losses we will use. The loss `nll` is the first term of the equation, whereas the `KLDivergenceLayer` is the second term of the loss. The `KLDivergenceLayer`will be used to compute an extra loss in the middle of the model via the `self.add_loss` function, but it does not change its inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvWDTeNodH78"
      },
      "outputs": [],
      "source": [
        "def nll(y_true, y_pred):\n",
        "  \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
        "\n",
        "  # keras.losses.binary_crossentropy gives the mean\n",
        "  # over the last axis. we require the sum\n",
        "  return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
        "\n",
        "class KLDivergenceLayer(Layer):\n",
        "\n",
        "  \"\"\" Identity transform layer that adds KL divergence\n",
        "  to the final model loss.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    self.is_placeholder = True\n",
        "    super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    mu, log_var = inputs\n",
        "    kl_batch = - .5 * K.sum(1 + log_var -\n",
        "                            K.square(mu) -\n",
        "                            K.exp(log_var), axis=-1)\n",
        "    self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUNQrpSjDC1Q"
      },
      "source": [
        "In the last block, we just defined the two losses we will use. Here we build the whole VAE model. First, we build the encoder. The goal of the encoder $\\phi$ is to approximate $P_\\theta(z|x_i)$ via $Q_\\phi(z|x_i)$.  We assume that $P(z)$ is a Normal distribution with zero mean and unit variance. We also assume that $Q(z|x)={N}(\\mu_x, \\sigma_x)$ so the encoder tries to recover the parameters $\\mu_x, \\sigma_x$ for the different $x$ (which are the input images), i.e. the encoder outputs for each latent dimension a mean and standard deviation. The code for this encoder is the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvOer91sEioL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# TF 2.0 broke this code, so we have to add this line\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "## Encoder\n",
        "# LeakyReLU are used in GANs generator as are usually\n",
        "# beneficial. We use it also in the VAE architecture\n",
        "# to match the GAN architecture, but ReLU works too.\n",
        "\n",
        "latent_dim = 2\n",
        "x = tf.keras.Input(shape=(original_dim,))\n",
        "h = Reshape((28, 28, 1))(x)\n",
        "h = Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same')(h)\n",
        "h = LeakyReLU(0.2)(h)\n",
        "h = Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same')(h)\n",
        "h = LeakyReLU(0.2)(h)\n",
        "h = Flatten()(h)\n",
        "\n",
        "## We recover here \\mu and \\sigma\n",
        "## For stability purposes, we assume that it outputs \\log(\\sigma)\n",
        "z_mu = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "## This layer adds the KL loss we defined before to the model\n",
        "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJKpM3JjEyRK"
      },
      "source": [
        "As we mentioned, the encoder will output the parameters $\\mu_x, \\sigma_x$. Now we will sample from the normal distribution defined by those parameters to pass it to the decoder. However, we now face one of the problems of implementing a VAE: we want to optimize both decoder and encoder at the same time to i) encourage good reconstruction and ii) to make $z$ follow a normal distribution. What is the problem here? Using a standard sampling method, i.e by directly sampling using the mean and standard deviation output by the encoder, we cannot train it in an end-to-end manner as sampling is not a differentiable operation.\n",
        "\n",
        "We need a trick to solve this issue. We can use one of the properties of the Normal probability distribution, which is that sampling $\\mathcal{N}(\\mu_\\psi, \\sigma_\\psi)$ is the same as $\\mu_\\psi + \\sigma_\\psi\\mathcal{N}(0, 1)$. Now, the sampling process is just a factor multiplying by the prediction $\\sigma_\\phi$ of the encoder, meaning we can propagate the gradients from the output back to the encoder. This is called the reparametrisation trick. We use this trick in the following block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruITB7tUc2gj"
      },
      "outputs": [],
      "source": [
        "##### Reparametrisation trick\n",
        "\n",
        "## Log_var to sigma\n",
        "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
        "\n",
        "## Sample using normal distribution\n",
        "eps = tf.keras.Input(tensor=K.random_normal(shape=(K.shape(x)[0], latent_dim)))\n",
        "\n",
        "## Multiply by sigma\n",
        "z_eps = Multiply()([z_sigma, eps])\n",
        "\n",
        "## Add mu\n",
        "z = Add()([z_mu, z_eps])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6MSQqJ3ot_r"
      },
      "source": [
        "Now we define the decoder, which will take the sampling from $\\mathcal{N}(\\mu_\\psi, \\sigma_\\psi)$ as input and output an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZxRuu2In_3E"
      },
      "outputs": [],
      "source": [
        "decoder = Sequential()\n",
        "decoder.add(Dense(128*7*7, input_dim=latent_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "decoder.add(LeakyReLU(0.2))\n",
        "decoder.add(Reshape((7, 7, 128)))\n",
        "decoder.add(UpSampling2D(size=(2, 2)))\n",
        "decoder.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
        "decoder.add(LeakyReLU(0.2))\n",
        "decoder.add(UpSampling2D(size=(2, 2)))\n",
        "decoder.add(Conv2D(1, kernel_size=(5, 5), padding='same', activation='sigmoid'))\n",
        "decoder.add(Flatten())\n",
        "x_pred = decoder(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LElZ521BrhrA"
      },
      "source": [
        "We have defined both the encoder and the decoder, and we are ready to train the model. We now build the model, which will have two inputs: the image `x`  for the encoder; and the sample `eps` (which refers to the sample from $\\mathcal{N}(0, 1)$ we mentioned before) for the decoder, which will use $\\mu_x, \\sigma_x$ via the reparametrisation trick."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqOeBPW6c3sj"
      },
      "outputs": [],
      "source": [
        "vae = Model(inputs=[x, eps], outputs=x_pred)\n",
        "vae.compile(optimizer='adam', loss=nll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZfdpBBrl2O"
      },
      "source": [
        "Now we train the model for some epochs to see if we can model our data. After the training process we will use this trained VAE to generate new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZd94z0Fc6WT",
        "outputId": "0ac6f229-16eb-4925-a36d-ca8031a48b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - ETA: 0s - loss: 173.0846"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 724s 12ms/sample - loss: 173.0846 - val_loss: 161.8952\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 690s 12ms/sample - loss: 160.0482 - val_loss: 159.4688\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 667s 11ms/sample - loss: 157.5633 - val_loss: 156.2144\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 691s 12ms/sample - loss: 156.0353 - val_loss: 154.9761\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 655s 11ms/sample - loss: 154.9542 - val_loss: 153.9545\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 681s 11ms/sample - loss: 154.0570 - val_loss: 155.3089\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 691s 12ms/sample - loss: 153.4012 - val_loss: 153.4429\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 684s 11ms/sample - loss: 153.0334 - val_loss: 153.4922\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 700s 12ms/sample - loss: 152.5885 - val_loss: 153.6485\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 692s 12ms/sample - loss: 152.2603 - val_loss: 152.0408\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 635s 11ms/sample - loss: 151.9814 - val_loss: 152.6873\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 682s 11ms/sample - loss: 151.5255 - val_loss: 152.4754\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 690s 12ms/sample - loss: 151.4117 - val_loss: 152.2297\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 685s 11ms/sample - loss: 151.2432 - val_loss: 151.9335\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 695s 12ms/sample - loss: 150.8968 - val_loss: 150.8401\n",
            "Epoch 16/20\n",
            "48200/60000 [=======================>......] - ETA: 2:13 - loss: 150.8172"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "batch_size = 50\n",
        "vae.fit(x_train,\n",
        "        x_train,\n",
        "        shuffle=True,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byac9m8MtpnI"
      },
      "source": [
        "Assuming we input into the decoder $\\mu_x$ obtained after encoding the image with the encoder, we can retrieve the MSE in the test set using the following piece of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsM1nhLPGwbw"
      },
      "outputs": [],
      "source": [
        "encoder = Model(x, z_mu)\n",
        "z_test = encoder.predict(x_test, batch_size=batch_size)\n",
        "images = decoder.predict(z_test)\n",
        "print(\"The MSE is: {:.4f}\".format(((images-x_test)**2).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz5MmFMzD48G"
      },
      "source": [
        "We trained an encoder to model $Q(z|x)$ which outputs two parameters per latent dimension for each image $x$, which are  $\\mu_x, \\sigma_x$. We now plot the distribution of the $\\mu_x$ when encoding the different images from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4H3u0n9dBLc"
      },
      "outputs": [],
      "source": [
        "encoder = Model(x, z_mu)\n",
        "\n",
        "# display a 2D plot of the digit classes in the latent space\n",
        "z_test = encoder.predict(x_test, batch_size=batch_size)\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "cm = plt.get_cmap('gist_rainbow')\n",
        "ax.set_prop_cycle(color=[cm(1.*i/(10)) for i in range(10)])\n",
        "for l in range(10):\n",
        "    # Only select indices for corresponding label\n",
        "    ind = y_test == l\n",
        "    ax.scatter(z_test[ind, 0], z_test[ind, 1], label=str(l),s=10)\n",
        "ax.legend()\n",
        "plt.title(\"Latent distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwjzHuHIEiMF"
      },
      "source": [
        "The distribution of the encoded means shows how they are clustered by class too, as in the autoencoder case. However, in this case, the distribution also follows a kind of circular distribution around the centre due to the Kullback Leibler divergence term in the loss. We made the $\\mu_x$ output by the encoder to be close to zero, and $\\sigma_x$ to be close to 1.\n",
        "\n",
        "Now let's start with the generation of data, which is the main reason we trained this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyplRgt6e3Lm"
      },
      "outputs": [],
      "source": [
        "# display a 2D manifold of the digits\n",
        "n = 15  # figure with 15x15 digits\n",
        "digit_size = 28\n",
        "\n",
        "# linearly spaced coordinates on the unit square were transformed\n",
        "# through the inverse CDF (ppf) of the Gaussian to produce values\n",
        "# of the latent variables z, since the prior of the latent space\n",
        "# is Gaussian\n",
        "\n",
        "z1 = norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z2 = norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z_grid = np.dstack(np.meshgrid(z1, z2))\n",
        "\n",
        "x_pred_grid = decoder.predict(z_grid.reshape(n*n, latent_dim)) \\\n",
        "                     .reshape(n, n, digit_size, digit_size)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(np.block(list(map(list, x_pred_grid))), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP7Il4dTeTAv"
      },
      "source": [
        "You can see in the image how there is a smooth transition between the different generated numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A36-s7n3tBpv"
      },
      "source": [
        "Now let's generate a nice animation for the latent variable, where we show the point we used in $z$ to generate the data and the corresponding image generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvcgjU-5irh0"
      },
      "outputs": [],
      "source": [
        "## We create a 2d array\n",
        "# Number of points to use, increase it for smoother animation\n",
        "# Using more points makes the function slower\n",
        "n_points = 50\n",
        "# theta from 0 to 2pi\n",
        "theta = np.linspace(0, 2*np.pi, n_points)\n",
        "# radius of the circle (change it depending on your reprentation space plot)\n",
        "r = 1\n",
        "# compute x and y (you can add an offset depending on your latent space)\n",
        "offset_x = 0\n",
        "offset_y = 0\n",
        "x = r*np.cos(theta) + offset_x\n",
        "y = r*np.sin(theta) + offset_y\n",
        "latent = np.stack([x, y], -1)\n",
        "\n",
        "## We now plot as before the 2d scatter with the images from the test set\n",
        "## and the corresponding label\n",
        "z_test = encoder.predict(x_test, batch_size=batch_size)\n",
        "fig, ax = plt.subplots(1,2)\n",
        "ax = fig.add_subplot(121)\n",
        "cm = plt.get_cmap('gist_rainbow')\n",
        "ax.set_prop_cycle(color=[cm(1.*i/(10)) for i in range(10)])\n",
        "for l in range(10):\n",
        "    # Only select indices for corresponding label\n",
        "    ind = y_test == l\n",
        "    ax.scatter(z_test[ind, 0], z_test[ind, 1], label=str(l),s=10)\n",
        "## This is needed to print the circle point (black point in the animation)\n",
        "scat = ax.scatter(latent[0,0], latent[0,1], s=200, c='k')\n",
        "\n",
        "## Create second plot with the generated image\n",
        "fig.add_subplot(122)\n",
        "latent_im = decoder.predict(latent)\n",
        "im = plt.imshow(latent_im[0].reshape(28, 28), animated=True, cmap='gray')\n",
        "def updatefig(i):\n",
        "    global latent_im\n",
        "    global latent, scat\n",
        "    scat.set_offsets(latent[i])\n",
        "    im.set_array(latent_im[i].reshape(28, 28))\n",
        "    return im,\n",
        "from matplotlib import animation\n",
        "anim = animation.FuncAnimation(fig, updatefig, interval=200, save_count=latent.shape[0])\n",
        "from IPython.display import HTML\n",
        "HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hc-XviqI44H"
      },
      "source": [
        "# Generative Adversarial Networks\n",
        "\n",
        "\n",
        "\n",
        "![](http://2018.igem.org/wiki/images/4/48/T--Vilnius-Lithuania-OG--introduction1.png)\n",
        "[Image source](https://2018.igem.org/Team:Vilnius-Lithuania-OG/Gan_Introduction)\n",
        "\n",
        "Generative Adversarial Networks have been shown to improve the generation of data compared to approaches such as VAE.\n",
        "\n",
        "As you learnt in the lecture, we have two networks playing what is called a min max game between them. The Generator, $G$, tries to generate data that looks similar to real data, whereas the discriminator $D$ tries to distinguish between real and fake data.\n",
        "\n",
        "Code adapted from [here](https://www.kdnuggets.com/2016/07/mnist-generative-adversarial-model-keras.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZJHL5nGTM-M"
      },
      "source": [
        "We first define $G$ here, and in this case we will use a convolutional network. Notice that we use what is called LeakyReLU for the activation functions, which have been shown to work well when using GANs. $G$ will take a vector of noise (in this case of dimensionality 10) sampled from $\\mathcal{N}(0, 1)$ and output a generated image. We use `tanh` as the last activation because the data will be normalized to be between $[-1, 1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvCu8786qNK1"
      },
      "outputs": [],
      "source": [
        "# Build Generative model\n",
        "# Optimizer\n",
        "adam = Adam(lr=0.0002, beta_1=0.5)\n",
        "randomDim = 5\n",
        "# Generator\n",
        "generator = Sequential()\n",
        "generator.add(Dense(128*7*7, input_dim=randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(Reshape((7, 7, 128)))\n",
        "generator.add(UpSampling2D(size=(2, 2)))\n",
        "generator.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(UpSampling2D(size=(2, 2)))\n",
        "generator.add(Conv2D(1, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
        "generator.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x2J0DyBNRW0"
      },
      "source": [
        "Here, we define the discriminator $D$. The discriminator is trained with the `categorical_crossentropy` loss, but you could also use a binary loss, as its job is to discriminate between real and fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8Ds7WLJqUGw"
      },
      "outputs": [],
      "source": [
        "# Build Discriminative model ...\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1), kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Flatten())\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCmi_IDwNasg"
      },
      "source": [
        "We defined the combined network, which combines the generator and discriminator in a network. The discriminator, however, will not be updated when using this combined network. We will explain later why, but basically we will use the `discriminator` object (not the combined network) whenever we need to update the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77v6wY77qupP"
      },
      "outputs": [],
      "source": [
        "# Combined network\n",
        "\n",
        "discriminator.trainable = False\n",
        "ganInput = Input(shape=(randomDim,))\n",
        "x = generator(ganInput)\n",
        "ganOutput = discriminator(x)\n",
        "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "gan.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQchMjunN0PJ"
      },
      "source": [
        "Now, let's define some helper functions that will be used to plot the loss and some generated images during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3kiJsWU2JXM"
      },
      "outputs": [],
      "source": [
        "def plot_loss(losses):\n",
        "    plt.figure()\n",
        "    plt.plot(losses[\"d\"], label='discriminitive loss')\n",
        "    plt.plot(losses[\"g\"], label='generative loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss.png')\n",
        "    plt.close()\n",
        "def plot_gen(mnist=1, n_ex=16, dim=(4,4), figsize=(10,10)):\n",
        "    noise = np.random.normal(0,1,size=[n_ex,randomDim])\n",
        "    generated_images = generator.predict(noise)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0],dim[1],i+1)\n",
        "        if mnist:\n",
        "          img = generated_images[i,:,:,0]\n",
        "          plt.imshow(img, cmap='gray')\n",
        "        else:\n",
        "          img = generated_images[i,:,:,:]\n",
        "          plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./images.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "razKxofMN60c"
      },
      "source": [
        "We will not use the regular fit method to train the model as in the usual approach. The reason for this is that the min max game that the GANs do, in practice is implemented in each of the batches by using two different training steps.\n",
        "* **First step - Training the discriminator:** In this step only the discriminator is trained.  The generator will output some fake images using noise as input. Then, we will give the discriminator these generated images and some images sampled from the real dataset and train it to distinguish between the two of them.\n",
        "\n",
        "* **Second step - Updating the generator**: We use the generator to output fake images again, and the discriminator will try to guess if these newly generated images are real or fake. However, the aim in this second step is not to update the discriminator, only the generator. This is why in the combined network we made the discriminator not to be trainable. The discriminator is used to pass information (in the form of gradients in this case) to update the generator. Hence, the generator will try to change its weights to make the discriminator think the data comes from the real distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg7X6vC5N6Y9"
      },
      "outputs": [],
      "source": [
        "def train_epoch(gan, generator, discriminator, plt_frq=25, BATCH_SIZE=32, mnist=1):\n",
        "  vector_ind = np.random.permutation(x_train.shape[0])\n",
        "  nb_epoch = int(x_train.shape[0]/BATCH_SIZE)\n",
        "  pbar = tqdm_notebook(range(nb_epoch))\n",
        "  for e in range(nb_epoch):\n",
        "    ind = vector_ind[e*BATCH_SIZE:(e+1)*BATCH_SIZE]\n",
        "    # Make generative images\n",
        "    image_batch = x_train[ind,:,:,:]\n",
        "    noise_gen = np.random.normal(0,1,size=[BATCH_SIZE,randomDim])\n",
        "    generated_images = generator.predict(noise_gen)\n",
        "    # Train discriminator on generated images\n",
        "    X = np.concatenate((image_batch, generated_images))\n",
        "    y = np.zeros([2*BATCH_SIZE])\n",
        "    y[0:BATCH_SIZE] = 1\n",
        "    y[BATCH_SIZE:] = 0\n",
        "\n",
        "    #make_trainable(discriminator,True)\n",
        "    d_loss  = discriminator.train_on_batch(X,y)\n",
        "    losses[\"d\"].append(d_loss)\n",
        "    # train Generator-Discriminator stack on input noise to non-generated output class\n",
        "    noise_tr = np.random.normal(0,1,size=[BATCH_SIZE,randomDim])\n",
        "    y2 = np.zeros([BATCH_SIZE])\n",
        "    y2[:] = 1\n",
        "\n",
        "    #make_trainable(discriminator,False)\n",
        "    g_loss = gan.train_on_batch(noise_tr, y2 )\n",
        "    losses[\"g\"].append(g_loss)\n",
        "\n",
        "    # Updates plots. This is a little bit of a mess due to how the notebook\n",
        "    # handles the outputs\n",
        "    if e % plt_frq==plt_frq-1:\n",
        "      plot_loss(losses)\n",
        "      plot_gen(mnist)\n",
        "      fig, ax = plt.subplots(2,1, figsize=(20,10) )\n",
        "      img=mpimg.imread('loss.png')\n",
        "      ax[0].imshow(img)\n",
        "      ax[0].axis('off')\n",
        "      img=mpimg.imread('images.png')\n",
        "      ax[1].imshow(img)\n",
        "      ax[1].axis('off')\n",
        "      plt.tight_layout()\n",
        "      display.clear_output(wait=True)\n",
        "      pbar.update(plt_frq)\n",
        "      display.display(pbar)\n",
        "      display.display(fig)\n",
        "      plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3EgAKi5IwIa"
      },
      "source": [
        "Let's start the training process. We will use MNIST again. A standard practice for GANs is to normalize images to the range $[-1, 1]$, which we do. We train it for 2 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tQm0vka2Vqd"
      },
      "outputs": [],
      "source": [
        "# set up loss storage vector\n",
        "losses = {\"d\":[], \"g\":[]}\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 127.5 - 1\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 127.5 - 1\n",
        "n_epoch = 10\n",
        "for i in range(n_epoch):\n",
        "  train_epoch(gan, generator, discriminator, plt_frq=200,BATCH_SIZE=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQPlGIFoI2Pv"
      },
      "source": [
        "The plots show how the training loss for both the generator and the discriminator does not change much. Usually we have a model which we optimize to reduce some metric/loss we pass it. However, in this case we have two models which are 'competing' against each other, so their losses are approximately stable. Now let's plot some results, as in the VAE case, we plot the results when sampling from 2 of the dimensions of the input noise $z$. However, as in this case we used more than 2 dimensions for $z$, the results vary due to choosing at random the 2 dimensions where to sample from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGTD49suQD5F"
      },
      "outputs": [],
      "source": [
        "# display a 2D manifold of the digits\n",
        "n = 15  # figure with 15x15 digits\n",
        "digit_size = 28\n",
        "\n",
        "# linearly spaced coordinates on the unit square were transformed\n",
        "# through the inverse CDF (ppf) of the Gaussian to produce values\n",
        "# of the latent variables z, since the prior of the latent space\n",
        "# is Gaussian\n",
        "\n",
        "z1 = norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z2 = norm.ppf(np.linspace(0.01, 0.99, n))\n",
        "z_grid = np.dstack(np.meshgrid(z1, z2))\n",
        "z_fill = np.random.normal(0,1,size=[randomDim-2])\n",
        "z_fill = np.expand_dims(z_fill, 0)\n",
        "z_fill = np.expand_dims(z_fill, 0)\n",
        "z_fill = np.repeat(z_fill, n, 0)\n",
        "z_fill = np.repeat(z_fill, n, 1)\n",
        "\n",
        "z_grid = np.concatenate([z_grid, z_fill], -1)\n",
        "np.random.shuffle(z_grid[:,:,-1])\n",
        "x_pred_grid = generator.predict(z_grid.reshape(n*n, randomDim)) \\\n",
        "                     .reshape(n, n, digit_size, digit_size)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(np.block(list(map(list, x_pred_grid))), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObF7kxiDR06j"
      },
      "source": [
        "**Training instability**\n",
        "\n",
        "GANs are quite difficult to train as there are several factors that can hurt their performance. Some of the most frequent issues are:\n",
        "\n",
        "* [Mode collapsing](https://www.youtube.com/watch?v=ktxhiKhWoEE): The generator is not capable of creating diverse images, it only generates a limited set of images.\n",
        "* Discriminator loss decreases quickly: in some cases the discriminator may be too powerful, so it quickly learns at the beginning which images are fake and which real, leading to small gradients passed to the generator.\n",
        "* Hyper parameter sensitivity\n",
        "* Non-convergence\n",
        "\n",
        "There are some tricks that have been shown to improve convergence, some of them are shared in this [repo](https://github.com/soumith/ganhacks).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENoiS-6xSYfy"
      },
      "source": [
        "# Inception Score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9bXrKeu6ZG_"
      },
      "source": [
        "The evaluation of a generative model quantitatively is a problem that is being research. The metric should evaluate the coverage, i.e. how diverse the generated data is, and sample quality, which is related to the visual quality of the sample. A widely used metric is the Inception Score, which is defined as:\n",
        "$$\n",
        "\\text{IS}(G)= \\exp(\\mathbb{E}_{x\\sim p_a}\\ D_{KL}(p(y|x)||p(y)))\n",
        "$$\n",
        "Let's explain the term a little bit. We aim to have a high IS, which means that we want a high $D_{KL}$ between $p(y|x)$ and $p(y)$. Ideally, what we want is $p(y|x)$ to be \"peaky\" and $p(y)$ to be uniform. $p(y|x)$ is the predicted probability vector $y$ given a generated sample $x$ by a model, in this case an Inception model (hence, the name of the score). If the sample $x$ is of high quality, the model should classify it with confidence in one of the available classes, hence the \"peaky\" $p(y|x)$. The term $p(y)$ is related to the diversity of the data, and is the marginal probability computed as $\\int_z p(y|x=G(z))dz$, hence we average the predicted probabilites of the Inception model for the given samples. If the data is diverse, we will see approximately obtain a flat $p(y)$.\n",
        "\n",
        "\n",
        "Code to compute the inception score adapted from [here](http://bluewidz.blogspot.com/2017/12/inception-score.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLgYEVrJtQz1"
      },
      "outputs": [],
      "source": [
        "!wget https://imperialcollegelondon.box.com/shared/static/5cc14wf0s4qwj65lec5852jlmxfy32m9.h5 -O inception_score_mnist.h5\n",
        "inception_score_model = keras.models.load_model('./inception_score_mnist.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK0VsondW5iU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from keras.preprocessing import image\n",
        "from keras.datasets import mnist\n",
        "from PIL import Image as pil_image\n",
        "\n",
        "\n",
        "def inception_score(x, resizer=None, batch_size=32, denorm_im=1):\n",
        "    r = None\n",
        "    n_batch = (x.shape[0]+batch_size-1) // batch_size\n",
        "    for j in range(n_batch):\n",
        "        x_batch = x[j*batch_size:(j+1)*batch_size, :, :, :]\n",
        "        if denorm_im:\n",
        "          x_batch = (x_batch + 1)/2\n",
        "        r_batch = inception_score_model.predict(x_batch) # r has the probabilities for all classes\n",
        "        r = r_batch if r is None else np.concatenate([r, r_batch], axis=0)\n",
        "    p_y = np.mean(r, axis=0) # p(y)\n",
        "    e = r*np.log(r/p_y) # p(y|x)log(P(y|x)/P(y))\n",
        "    e = np.sum(e, axis=1) # KL(x) = Σ_y p(y|x)log(P(y|x)/P(y))\n",
        "    e = np.mean(e, axis=0)\n",
        "    return np.exp(e) # Inception score\n",
        "\n",
        "\n",
        "def image_inception_score(generator, n_ex=10000, dim_random=10, input_noise=None, denorm_im=1):\n",
        "    if input_noise is None:\n",
        "      input_noise = np.random.normal(0,1,size=[n_ex,dim_random])\n",
        "    x_pred = generator.predict(input_noise)\n",
        "    if len(x_pred.shape)==2:\n",
        "      x_pred = x_pred.reshape(n_ex, 28, 28, 1)\n",
        "    return inception_score(x_pred, denorm_im=denorm_im)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHTkEqtOHo2D"
      },
      "source": [
        "Now let's check the Inception Score for the GAN model we have trained before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eSlOzotwHw_"
      },
      "outputs": [],
      "source": [
        "image_inception_score(generator, dim_random=randomDim, denorm_im=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDwVU-M3sAkN"
      },
      "source": [
        "Due to the way we defined the generator for the GAN, images are generated in the range $[-1, 1]$ as we use a `tanh` activation function. That is why we use the `denorm_im` variable to map them to the range $[0, 1]$.\n",
        "\n",
        "The decoder of the VAE can also be evaluated using the same `image_inception_score` function. A VAE uses a prior $\\mathcal{N}(0,1)$ as an extra constraint in the loss, hence if we sample from $\\mathcal{N}(0,1)$ as done in `image_inception_score`, the decoder should be able to generate new samples. When using the function for the VAE, use `image_inception_score(..., denorm_im=0)` as the decoder in the VAE already uses the range $[0,1]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCVwckWQLHno"
      },
      "source": [
        "# Conditional Generative Adversarial Networks (cGANs)\n",
        "\n",
        "A Conditional GAN (cGAN) is a variant of classical Generative Adversarial Networks, where the task is conditioned on some extra information instead of random noise. They were introduced in 2014, [link to the cGANs paper](https://arxiv.org/abs/1411.1784), and set a new state of the art in many image tasks.\n",
        "\n",
        "In this setting, cGANs expect an input image instead of a random noise vector. A simple example of image translation is the drawing to real object transformation. To do so, imagine that we have two different paired domains, images of the real objects (domain A) and drawing of objects (domain B), and we want to go from one to the other. In classical GANs, we used these two domains for training the architectures, but here we can also condition the input to one of the images in the domain B, and use the relationship between the domains to guide the network to the desired result:\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/rF2w5Tt/Screenshot-from-2019-02-25-15-47-18.png\" alt=\"Screenshot-from-2019-02-25-15-47-18\" border=\"0\"></a>\n",
        "\n",
        "As in regular GANs, the discriminator learns to classify between fake (synthesized by the generator) and real {drawing, photo} tuples. Hence, the generator will try to fool the discriminator with better and better synthesised images. In contrast to GANS, as we feed the drawing directly to the generator, the information that the network can use to seek convergence is much wider."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B3XO3spH0ou"
      },
      "source": [
        "## Image-to-image Translation with cGANs\n",
        "\n",
        "\n",
        "[Image-to-image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) is a paper presented at the Computer Vision and Pattern Recognition (CVPR) conference in 2017. In their paper, the authors introduced Pix2pix, a conditional GAN, to transform images from one domain to another. The authors in the paper investigated conditional adversarial networks as a general-purpose solution to image-to-image translation problems, see some examples from the paper of different Image-to-Image tasks:\n",
        "\n",
        ">\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/Mf08787/examples.jpg)\n",
        "\n",
        "In this tutorial, we will explain and implement Pix2pix model for colouring images step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrEuqkIINKxi"
      },
      "source": [
        "## Colouring Black and White Images\n",
        "The tutorial aims to build the Pix2pix code for colouring images. We will use CIFAR10 dataset to perform the experiment, however, you could try to use any other dataset. For instance, check [this repo](https://github.com/kvfrans/deepcolor) which shows how to colour manga-style images.\n",
        "\n",
        "The idea is quite intuitive, first of all, we will transform images into black and white (BW) and train a generator that will transform them back to RGB images. As in classical GANs, we use the discriminator to differentiate between *Fake* and *Real* images.\n",
        "\n",
        "Thus, we have domain A (colour) and domain B (B&W)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zaEwTyYXcrj"
      },
      "source": [
        "## Dataset Generator\n",
        "\n",
        "When training a cGAN, or any regular GAN architecture, we need to train independently the generator and discriminator networks. As seen, the training is a min-max game between the two nets.\n",
        "\n",
        "Firstly, we aim to train the generator to be able to fool the discriminator, while in the next iteration, we will train the discriminator to be able to distinguish between the generated (fake) images and the real ones.\n",
        "\n",
        "We are going to switch the training between networks in every step. Instead of training per a whole epoch as has been seen in the previous tutorials, we will generate batches and train first the discriminator and then switch to train the generator.\n",
        "\n",
        "To accomplish that, we need to use the module provided by Keras *model.train_on_batch()* and code our training loop. In this case, we can not rely on Keras *model.fit()* module to handle the dataset as done previously. Moreover, we need to create a dataset generator to feed with batches our training loop.\n",
        "\n",
        "Let's define our class *DataLoader()*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r7Y-RLnTlqw"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10, cifar100\n",
        "from keras.layers import Lambda, Input\n",
        "from keras.models import Model\n",
        "import tensorflow as ktf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "class DataLoader():\n",
        "  def __init__(self, dataset_name, img_res=(32, 32)):\n",
        "    self.dataset_name = dataset_name\n",
        "    self.img_res = img_res\n",
        "    self.load_dataset()\n",
        "\n",
        "  def _start_epoch(self):\n",
        "\n",
        "    random_idx = np.random.permutation(len(self.im_A_train))\n",
        "    self.im_A_train_epoch = self.im_A_train[random_idx]\n",
        "\n",
        "  def load_dataset(self):\n",
        "\n",
        "    if self.dataset_name == 'CIFAR10':\n",
        "      (im_A_train, y_train), (im_A_test, y_test) = cifar10.load_data()\n",
        "    elif self.dataset_name == 'CIFAR100':\n",
        "      (im_A_train, y_train), (im_A_test, y_test) = cifar100.load_data()\n",
        "    else:\n",
        "      raise Exception('Please, select a valid dataset')\n",
        "\n",
        "    self.im_A_train = im_A_train.astype('float32')\n",
        "    self.im_A_test = im_A_test.astype('float32')\n",
        "    self.im_A_train /= 255.\n",
        "    self.im_A_test /= 255.\n",
        "\n",
        "  def get_dataset_shape(self, is_training=True):\n",
        "\n",
        "    if is_training:\n",
        "      return self.im_A_train.shape\n",
        "    else:\n",
        "      return self.im_A_test.shape\n",
        "\n",
        "  def get_num_batches(self, batch_size):\n",
        "    return int(self.im_A_train.shape[0] / batch_size)\n",
        "\n",
        "  def set_image_transformations(self, convert_to_bw):\n",
        "\n",
        "    self.convert_to_bw = convert_to_bw\n",
        "\n",
        "  def load_batch(self, batch_size=1, is_training=True):\n",
        "\n",
        "    if is_training:\n",
        "\n",
        "      self._start_epoch()\n",
        "      num_batches = int((self.im_A_train).shape[0] / batch_size)\n",
        "\n",
        "    else:\n",
        "      num_batches = int((self.im_A_test).shape[0] / batch_size)\n",
        "\n",
        "    for idx_batch in range(num_batches):\n",
        "\n",
        "      init = idx_batch * batch_size\n",
        "      end = (idx_batch + 1) * batch_size\n",
        "\n",
        "      if is_training:\n",
        "        batch = self.im_A_train_epoch[init:end]\n",
        "      else:\n",
        "        batch = self.im_A_test[init:end]\n",
        "\n",
        "      batch = np.reshape(batch, (batch_size, self.img_res[0], self.img_res[1], 3))\n",
        "\n",
        "      # Convert to B&W\n",
        "      batch_bw = self.convert_to_bw.predict(batch)\n",
        "\n",
        "      yield [batch, batch_bw]\n",
        "\n",
        "  def get_random_batch(self, batch_size=1, is_training=True):\n",
        "\n",
        "    if is_training:\n",
        "      random_idx = np.random.permutation(len(self.im_A_train))\n",
        "      batch = self.im_A_train[random_idx[0:batch_size]]\n",
        "    else:\n",
        "      random_idx = np.random.permutation(len(self.im_A_test))\n",
        "      batch = self.im_A_test[random_idx[0:batch_size]]\n",
        "\n",
        "    batch = np.reshape(batch, (batch_size, self.img_res[0], self.img_res[1], 3))\n",
        "\n",
        "    # Convert to B&W\n",
        "    batch_bw = self.convert_to_bw.predict(batch)\n",
        "\n",
        "    return [batch, batch_bw]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDfgTUXZZM_G"
      },
      "source": [
        "*DataLoader* class needs only the name of the dataset to load. Now, it can load CIFAR10 and CIFAR100. Let's see what some of *DataLoader*'s methods do:\n",
        "\n",
        "\n",
        "*   *_start_epoch()* is a private method. It shuffles the dataset every time that *load_batch()* is called.\n",
        "*   *load_dataset()* loads the dataset into *DataLoader* object.\n",
        "*  *load_batch()* creates a python generator. In each iteration, it will return a batch from the shuffled dataset.\n",
        "\n",
        "Let's now define the dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMe042E2cDH7"
      },
      "outputs": [],
      "source": [
        "# Load the data, shuffled and split between train and test sets\n",
        "dataset_loader = DataLoader(dataset_name = 'CIFAR10')\n",
        "\n",
        "training_shape = dataset_loader.get_dataset_shape()\n",
        "test_shape = dataset_loader.get_dataset_shape(is_training=False)\n",
        "\n",
        "print('Shape of Training Images: {}'.format(training_shape))\n",
        "print('Shape of Test Images: {}'.format(test_shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGKwoeF2UZ5U"
      },
      "source": [
        "## Defining the Domains\n",
        "\n",
        "As explained above, we will go from B&W to RGB colour images. To get the B&W images, we need to define *transform_bw*, a method that will transform images from **domain A** (colour) to images in **domain B** (B&W). We can do that with lambda functions as seen in the *CNN Introduction* tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpqU5D3Icdo4"
      },
      "outputs": [],
      "source": [
        "# Convert to greyscale color space\n",
        "inp = Input(shape=(None, None, 3))\n",
        "out = Lambda(lambda image: ktf.compat.v1.image.rgb_to_grayscale(image))(inp)\n",
        "\n",
        "transform_bw = Model(inputs=inp, outputs=out)\n",
        "\n",
        "# Set predefined transformation in DataLoader Class\n",
        "dataset_loader.set_image_transformations(transform_bw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7JgJGacphw"
      },
      "source": [
        "We can visualise images from both domains before starting to code our cGAN. Rerun the following code to check different examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7Of7_EXLHSH"
      },
      "outputs": [],
      "source": [
        "# Load random batch from dataset\n",
        "random_batch = dataset_loader.get_random_batch(batch_size=9)\n",
        "\n",
        "# Repeat last dimension for visualization\n",
        "tmp = np.repeat(random_batch[1], 3, axis=3)\n",
        "\n",
        "N=3\n",
        "start_val = 0\n",
        "fig, axes = plt.subplots(N,N)\n",
        "plt.suptitle('Domain A VS Domain B', fontsize=18)\n",
        "for row in range(N):\n",
        "  for col in range(N):\n",
        "    idx = start_val+row+N*col\n",
        "\n",
        "    im = np.concatenate((random_batch[0][idx], tmp[idx]), 1)\n",
        "    axes[row,col].imshow(np.clip(im, 0, 1))\n",
        "    axes[row,col].set_xticks([])\n",
        "    axes[row,col].set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUIdgtbpUg6j"
      },
      "source": [
        "## Generator & Discriminator Models\n",
        "\n",
        "For many image translation problems, there is a great deal of low-level information shared between the input and output. Thus, it is desirable to shuttle this information directly across the net. For example, in the case of image colourization, the input and output share the location of prominent edges. To give the generator a means to circumvent the bottleneck for information like this, we use an architecture with skip connections, following the general shape of the UNet introduced in previous tutorials.\n",
        "\n",
        "The generator introduced in this tutorial is a simpler version of the one in the paper since we are using low-resolution images (32x32).\n",
        "\n",
        "In Pix2pix, the authors found that mixing the GAN objective with a more traditional loss, such as L1 loss, improved the final performance. The discriminator’s job remains unchanged, but the generator is tasked to not only fool the discriminator but also to be near the ground-truth output in an L1 sense.\n",
        "\n",
        "We can define our cGAN generator model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzkpbJwwttVD"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dropout, merge, concatenate, UpSampling2D, MaxPooling2D\n",
        "from keras.layers import Conv2D, Dense, Reshape, Flatten, Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Conv2D, Input, Dense, Reshape, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def build_generator(im_shape):\n",
        "\n",
        "  img_B = Input(shape=(im_shape[0], im_shape[1], 1))\n",
        "\n",
        "  ## Encoder part\n",
        "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(img_B)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "  drop4 = Dropout(rate=0.5)(conv4)\n",
        "  pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "  drop5 = Dropout(rate=0.5)(conv5)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "  up6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "  merge6 = concatenate([drop4,up6], axis = 3)\n",
        "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "\n",
        "  up7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "  merge7 = concatenate([conv3,up7], axis = 3)\n",
        "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "\n",
        "  up8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "  merge8 = concatenate([conv2,up8], axis = 3)\n",
        "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "\n",
        "  up9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "  merge9 = concatenate([conv1,up9], axis = 3)\n",
        "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "  conv10 = Conv2D(3, 3,  padding = 'same')(conv9)\n",
        "\n",
        "  model = Model(inputs = img_B, outputs = conv10, name='generator')\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkhWs2Fwztka"
      },
      "source": [
        "Next, we follow the discriminator model definition from the paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usNum3W7zvar"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(im_shape):\n",
        "\n",
        "  img_A = Input(shape=(im_shape[0], im_shape[1], 3))\n",
        "  img_B = Input(shape=(im_shape[0], im_shape[1], 1))\n",
        "\n",
        "  combined_imgs = concatenate([img_A, img_B], axis=-1)\n",
        "\n",
        "  disc_layer = Conv2D(64, kernel_size=(5, 5), padding='same', strides=(2, 2))(combined_imgs)\n",
        "  disc_layer = LeakyReLU(alpha=0.2)(disc_layer)\n",
        "\n",
        "  disc_layer = Conv2D(64, kernel_size=(5, 5), padding='same', strides=(2, 2))(disc_layer)\n",
        "  disc_layer = BatchNormalization(momentum=0.8)(disc_layer)\n",
        "  disc_layer = LeakyReLU(alpha=0.2)(disc_layer)\n",
        "\n",
        "  disc_layer = Conv2D(128, kernel_size=(5, 5), strides=(2, 2))(disc_layer)\n",
        "  disc_layer = BatchNormalization(momentum=0.8)(disc_layer)\n",
        "  disc_layer = LeakyReLU(alpha=0.2)(disc_layer)\n",
        "\n",
        "  disc_layer = Flatten()(disc_layer)\n",
        "  disc_layer = Dense(1024)(disc_layer)\n",
        "\n",
        "  prob = Dense(1, name=\"disc_dense\")(disc_layer)\n",
        "\n",
        "  discriminator = Model(inputs=[img_A, img_B], outputs=[prob], name='discriminator')\n",
        "\n",
        "  return discriminator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etn3wYwi8A2k"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "We arrive at the tricky part of the cGAN tutorial, where we need to define the training loop of our model. Do not focus much on this part of the code, this is only provided in case you want to further train the architectures.\n",
        "\n",
        "First, we need to define the optimisers, inputs, outputs, and losses for each one of the networks. Let's go line by line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP7wTglB8Jxf"
      },
      "outputs": [],
      "source": [
        "# Define optimizers for each network.\n",
        "# Note that if the discriminator is able to differentiate between samples,\n",
        "# the generator will be not able to learn. They must learn at the same time.\n",
        "# Setting the learning rates or learning steps is always tricky.\n",
        "optimizer_g = Adam(0.0002, 0.5)\n",
        "optimizer_d = Adam(0.00002, 0.5)\n",
        "\n",
        "# Input size\n",
        "im_shape = (32, 32)\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator(im_shape)\n",
        "discriminator.compile(loss='mse', optimizer=optimizer_d, metrics=['accuracy'])\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator(im_shape)\n",
        "\n",
        "# Input images and their conditioning images\n",
        "img_A = Input(shape=(im_shape[0], im_shape[1], 3))\n",
        "img_B = Input(shape=(im_shape[0], im_shape[1], 1))\n",
        "\n",
        "# By conditioning on B generate a fake version of A.\n",
        "# Remember, images in B are the B&W version of images in A\n",
        "fake_A = generator(img_B)\n",
        "\n",
        "# For the combined model we will only train the generator, thus, we freeze the\n",
        "# discriminator model when optimizing G.\n",
        "# We are backpropagating the discriminator's error\n",
        "# into the generator in order to create fake images that are unrecognizable by D\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Discriminators determines if a conditioned pair has been generated or is real\n",
        "valid = discriminator([fake_A, img_B])\n",
        "\n",
        "# Minimize discriminator error (only updating generator's weights) and\n",
        "# the L1 loss between real and fake images.\n",
        "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
        "combined.compile(loss=['mse', 'mae'], loss_weights=[1, 100], optimizer=optimizer_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNzo7ijs2THo"
      },
      "source": [
        "We have created the models and compiled them. Besides, before starting the training loop, we are going to define two auxiliary functions that will allow us to print images during training to have an idea of how well the generator is colouring the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpZvidvfSdrC"
      },
      "outputs": [],
      "source": [
        "def showColoredIms(imB, fake_imA, real_imA):\n",
        "\n",
        "  plt.subplot(131)\n",
        "  plt.imshow(np.clip(imB[0], 0, 1)[:,:,0], cmap='gray')\n",
        "  plt.title('Domain B', fontsize=20)\n",
        "  plt.gca().set_xticks([])\n",
        "  plt.gca().set_yticks([])\n",
        "  plt.subplot(132)\n",
        "  plt.imshow(np.clip(fake_imA[0], 0, 1))\n",
        "  plt.title('Fake A', fontsize=20)\n",
        "  plt.gca().set_xticks([])\n",
        "  plt.gca().set_yticks([])\n",
        "  plt.subplot(133)\n",
        "  plt.imshow(np.clip(real_imA[0], 0, 1))\n",
        "  plt.title('Real A', fontsize=20)\n",
        "  plt.gca().set_xticks([])\n",
        "  plt.gca().set_yticks([])\n",
        "  plt.show()\n",
        "\n",
        "def showColored_two_models_Ims(imB, fake_imA_MAE, fake_imA_cGAN, real_imA):\n",
        "\n",
        "  plt.subplot(141)\n",
        "  plt.imshow(np.clip(imB[0], 0, 1)[:,:,0], cmap='gray')\n",
        "  plt.title('BW', fontsize=20)\n",
        "  plt.gca().set_xticks([])\n",
        "  plt.gca().set_yticks([])\n",
        "  plt.subplot(142)\n",
        "  plt.imshow(np.clip(fake_imA_MAE[0], 0, 1))\n",
        "  plt.title('MAE', fontsize=20)\n",
        "  plt.gca().set_xticks([])\n",
        "  plt.gca().set_yticks([])\n",
        "  plt.subplot(143)\n",
        "  plt.imshow(np.clip(fake_imA_cGAN[0], 0, 1))\n",
        "  plt.title('cGAN', fontsize=20)\n",
        "  plt.gca().set_xticks([])\n",
        "  plt.gca().set_yticks([])\n",
        "  plt.subplot(144)\n",
        "  plt.imshow(np.clip(real_imA[0], 0, 1))\n",
        "  plt.title('Real', fontsize=20)\n",
        "  plt.gca().set_xticks([])\n",
        "  plt.gca().set_yticks([])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLWgeEV5dT5"
      },
      "source": [
        "In the next section, we provide trained weights so you could resume or skip the training. These weights were obtained by training the net for 20 epochs. Note that if you want to further train the networks, GAN training is characterized for being really long.\n",
        "\n",
        "Let's see the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhMXE6cEKEvV"
      },
      "outputs": [],
      "source": [
        "num_epochs = 1\n",
        "batch_size = 128\n",
        "n_batches = dataset_loader.get_num_batches(batch_size)\n",
        "\n",
        "# Adversarial loss ground truths.\n",
        "# They are used as labels for the discriminator loss.\n",
        "valid = np.ones((batch_size,))\n",
        "fake = np.zeros((batch_size,))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  start_time = datetime.datetime.now()\n",
        "\n",
        "  # Record average losses. Monitorize the loss function.\n",
        "  g_avg_loss = []\n",
        "  d_avg_loss = []\n",
        "  d_avg_acc = []\n",
        "\n",
        "  # load_batch() returns a batch generator\n",
        "  # Before starting the epoch, it shuffles the dataset\n",
        "  for batch_i, [imgs_A, imgs_B] in enumerate(dataset_loader.load_batch(batch_size)):\n",
        "\n",
        "    # ---------------------\n",
        "    #  Train Discriminator\n",
        "    # ---------------------\n",
        "\n",
        "    # Condition on B and generate a translated (fake) version of A.\n",
        "    # It will try to color images from B to be as similiar as possible\n",
        "    # to their correspoding pairs from A.\n",
        "    fake_A = generator.predict(imgs_B)\n",
        "\n",
        "    # Train the discriminator (original images = real / generated = Fake)\n",
        "    d_loss_real = discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
        "    d_loss_fake = discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    d_avg_loss.append(d_loss[0])\n",
        "    d_avg_acc.append(d_loss[1])\n",
        "\n",
        "    # -----------------\n",
        "    #  Train Generator\n",
        "    # -----------------\n",
        "\n",
        "    # Train the generators\n",
        "    g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
        "    g_avg_loss.append(g_loss[0])\n",
        "\n",
        "    elapsed_time = datetime.datetime.now() - start_time\n",
        "\n",
        "    # Aproximation of epoch remaining time\n",
        "    remaining_time = (elapsed_time/(batch_i+1)) * (n_batches-batch_i-1)\n",
        "\n",
        "    # Plot examples\n",
        "    if batch_i%50 == 0:\n",
        "      showColoredIms(imgs_B, fake_A, imgs_A)\n",
        "\n",
        "    # Plot the progress\n",
        "    if batch_i%10 == 0:\n",
        "\n",
        "      print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] elapsed_time: %s  remaining_time: %s\" % (epoch, num_epochs,\n",
        "              batch_i, n_batches, np.mean(d_avg_loss), 100*np.mean(d_avg_acc), np.mean(g_avg_loss), elapsed_time, remaining_time))\n",
        "\n",
        "  # Saves optimizer and weights\n",
        "  generator.save('generator.h5')\n",
        "  discriminator.save('discriminator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WC5CPlk2x-V"
      },
      "source": [
        "If desired, you could download the model you just trained. Those weights can be used for resuming future trainings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVftcAJb2uiU"
      },
      "outputs": [],
      "source": [
        "from google.colab  import files\n",
        "\n",
        "# Download the weights in your PC\n",
        "files.download('generator.h5')\n",
        "files.download('discriminator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8aVvc8k2_NP"
      },
      "source": [
        "## Colouring Test Images\n",
        "\n",
        "We are ready to visualise how the network colours the test images. If you have not to train your model, you can optionally load the provided trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ExvhyT5Oxh"
      },
      "outputs": [],
      "source": [
        "# Load weights from a previous session\n",
        "# Upload weights to Colab by using the File tab\n",
        "\n",
        "!wget -O weights.zip https://imperialcollegelondon.box.com/shared/static/w2m93zcadbycpspaq4jifug9bo8l88ad.zip\n",
        "!unzip -q ./weights.zip\n",
        "!rm ./weights.zip\n",
        "\n",
        "generator = keras.models.load_model('./weights/generator.h5')\n",
        "discriminator = keras.models.load_model('./weights/discriminator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apZzJgmM-2oD"
      },
      "source": [
        "Let's see the results, you can rerun the following code for visualizing more examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ljS-uv1jMI"
      },
      "outputs": [],
      "source": [
        "# Load random batch from dataset\n",
        "\n",
        "for i in range(3):\n",
        "  [im_A_real, im_B_test] = dataset_loader.get_random_batch(batch_size=1, is_training=False)\n",
        "\n",
        "  im_A_fake = generator.predict(im_B_test)\n",
        "\n",
        "  showColoredIms(im_B_test, im_A_fake, im_A_real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QfiXDhB3TSi"
      },
      "source": [
        "The colouring may not be perfect, but the network does a good job at guessing plausible colours. We have shown a simplified version of the Pix2pix method. Training GANs is a hard task, and there are many elements that could be added to improve results. For instance, [this repo](https://github.com/soumith/ganhacks), like many others, shows common tricks to improve GAN performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcG_R3OAGe5m"
      },
      "source": [
        "# Coursework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpQekp8aNVt_"
      },
      "source": [
        "\n",
        "## Task 1: MNIST generation using VAE and GAN\n",
        "\n",
        "**Report**\n",
        "* Train the given VAE model in the tutorial also using the same hyper parameters (batch size, optimizer, number of epochs, etc...) but increasing the latent dimensionality to 10. Compute the MSE on the reconstructed `x_test` images and Inception Score (IS). Now train the same model without the KL divergence loss, and compute again the MSE and IS score. Report the results in a table and discuss them. For the IS you can use in this case `image_inception_score(decoder, dim_random=10, denorm_im=0)`.\n",
        "\n",
        "* Train the GAN given in the tutorial with increased dimensionality of the initial random sample to 10 for 10 epochs and report its IS (use the same table as in the VAE case). Discuss the difference in the obtained IS for VAE and GAN and link it to the qualitative results. For the IS use in this case `image_inception_score(generator, dim_random=10, denorm_im=1)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkjwCF6uw3K"
      },
      "source": [
        "## Task 2: Quantitative VS Qualitative Results\n",
        "\n",
        "In this task, we will observe the difference between two trained models for colouring images. One is the model trained during the tutorial, which uses a cGAN approach to predict the RGB pixel-wise values of a B&W image. The other one is a simple UNet autoencoder trained with a Mean Absolute Error (MAE) loss, which is trained to predict directly the RBG image without any GAN based learning strategy. We refer to the first and second models as cGAN and MAE models, respectively. For this task, 20 epochs trained weights for the cGAN and MAE models are provided. If desired, the code to train the MAE model can be found below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCKFnMxkOMyS"
      },
      "outputs": [],
      "source": [
        "# Input images and their conditioning images\n",
        "img_A = Input(shape=(im_shape[0], im_shape[1], 3))\n",
        "img_B = Input(shape=(im_shape[0], im_shape[1], 1))\n",
        "\n",
        "# Build the generator\n",
        "generator_mae = build_generator(im_shape)\n",
        "fake_A = generator_mae(img_B)\n",
        "generator_mae = Model(inputs=img_B, outputs=fake_A)\n",
        "generator_mae.compile(optimizer = 'Adam', loss = 'mean_absolute_error', metrics = ['mae'])\n",
        "\n",
        "num_epochs = 1\n",
        "batch_size = 128\n",
        "n_batches = dataset_loader.get_num_batches(batch_size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  start_time = datetime.datetime.now()\n",
        "\n",
        "  # Record average losses. Monitorize the loss function.\n",
        "  g_avg_loss = []\n",
        "\n",
        "  # load_batch() returns a batch generator\n",
        "  # Before starting the epoch, it shuffles the dataset\n",
        "  for batch_i, [imgs_A, imgs_B] in enumerate(dataset_loader.load_batch(batch_size)):\n",
        "\n",
        "    # -----------------\n",
        "    #  Train Generator (MAE)\n",
        "    # -----------------\n",
        "    g_loss = generator_mae.train_on_batch(imgs_B, imgs_A)\n",
        "    g_avg_loss.append(g_loss[0])\n",
        "\n",
        "    fake_A = generator_mae.predict(imgs_B)\n",
        "\n",
        "    elapsed_time = datetime.datetime.now() - start_time\n",
        "\n",
        "    # Aproximation of epoch remaining time\n",
        "    remaining_time = (elapsed_time/(batch_i+1)) * (n_batches-batch_i-1)\n",
        "\n",
        "    # Plot examples\n",
        "    if batch_i%50 == 0:\n",
        "      showColoredIms(imgs_B, fake_A, imgs_A)\n",
        "\n",
        "    # Plot the progress\n",
        "    if batch_i%10 == 0:\n",
        "\n",
        "      print (\"[Epoch %d/%d] [Batch %d/%d] [G loss: %f] elapsed_time: %s  remaining_time: %s\" % (epoch, num_epochs,\n",
        "              batch_i, n_batches, np.mean(g_avg_loss), elapsed_time, remaining_time))\n",
        "\n",
        "  # Saves optimizer and weights\n",
        "  generator_mae.save('generator_mae.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9aTGM7qwtyO"
      },
      "source": [
        "Instead of training the models, we can directly load their pre-trained weights by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbVvCIQyw0B8"
      },
      "outputs": [],
      "source": [
        "!wget -O weights.zip https://imperialcollegelondon.box.com/shared/static/w2m93zcadbycpspaq4jifug9bo8l88ad.zip\n",
        "!unzip -q ./weights.zip\n",
        "!rm ./weights.zip\n",
        "\n",
        "!wget -O weights_mae.zip https://imperialcollegelondon.box.com/shared/static/empxfp2v05xp7h8rwsccuiy35c3vddzd.zip\n",
        "!unzip -q ./weights_mae.zip\n",
        "!rm ./weights_mae.zip\n",
        "\n",
        "generator_cGAN = keras.models.load_model('./weights/generator.h5')\n",
        "generator_mae = keras.models.load_model('mae_generator.h5', compile=False)\n",
        "\n",
        "generator_cGAN.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['mae'])\n",
        "generator_mae.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyXI9hCTzUn2"
      },
      "source": [
        "We have loaded both models, and we are ready to compare them. In this task, you are asked to analyse the difference between the quantitative versus the qualitative results. To do so, we provided two pieces of code. The first one will compute the MAE metric for both models in the test dataset. As we know, this metric is widely used on image generation tasks, such as image upsampling, image reconstruction, image translation, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t46neFcx6nyg"
      },
      "outputs": [],
      "source": [
        "g_mae_avg_mae = []\n",
        "for batch_i, [imgs_A, imgs_B] in enumerate(dataset_loader.load_batch(128, is_training=False)):\n",
        "    _, mae = generator_mae.evaluate(imgs_B, imgs_A, verbose=0)\n",
        "    g_mae_avg_mae.append(mae)\n",
        "\n",
        "print(\"MAE (Trained MAE): {:.4f}\".format(np.mean(g_mae_avg_mae)))\n",
        "\n",
        "g_cgan_avg_mae = []\n",
        "for batch_i, [imgs_A, imgs_B] in enumerate(dataset_loader.load_batch(128, is_training=False)):\n",
        "    _, mae = generator_cGAN.evaluate(imgs_B, imgs_A, verbose=0)\n",
        "    g_cgan_avg_mae.append(mae)\n",
        "\n",
        "print(\"MAE (Trained cGAN): {:.4f}\".format(np.mean(g_cgan_avg_mae)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddZgs-Il8F9B"
      },
      "source": [
        "The next piece of code will show coloured examples for both networks, so you can check them visually and discuss which model is better. First, we need to create an iterator object to go through the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAVQ-Dyj8ECJ"
      },
      "outputs": [],
      "source": [
        "iterator = iter(dataset_loader.load_batch(1, is_training=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjTR797V8eys"
      },
      "source": [
        "Run multiple examples so that you have a clear idea of how both methods differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-FC55MDy096"
      },
      "outputs": [],
      "source": [
        "# Load test example\n",
        "[imgs_A, imgs_B] = next(iterator)\n",
        "\n",
        "# Generate predictions for both models\n",
        "fake_A_cGAN = generator_cGAN.predict(imgs_B)\n",
        "fake_A_MAE = generator_mae.predict(imgs_B)\n",
        "\n",
        "# Plot all images\n",
        "showColored_two_models_Ims(imgs_B, fake_A_MAE, fake_A_cGAN, imgs_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss_1bTVE-Us_"
      },
      "source": [
        "We showed that both models obtain a similar MAE value. If we would only take into account the quantitative metric, as done in many scientific articles, we would say that the MAE model is better. However, in addition to the quantitative results, we need to analyse visually the results produced by the two networks to declare which is the best model.\n",
        "\n",
        "**Report**\n",
        "\n",
        "\n",
        "*   Run the previous code to analyse several coloured images for both models. Based on previous results and linked to GAN theory, discuss from the numerical and visual perspective if both models are similar, or whether there is a better one. You can provide in the report visual examples together with their MAE values to support your arguments. The figure of this task can be included in the Appendix. Discussion still needs to go into the main text."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}